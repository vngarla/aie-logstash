input {
	# File is a plugin for reading in files from disk. Who would have thought.
	file {
		# Adds a field to each input event called 'type' with the value 'querylog'
		type => "querylog"
		# The path to the logs
		path => "c:/code/data/aie_4.3.0/data-agent/projects/factbook430/default/logs/logs-local/querylogs/*.log"
		# Where to write the since database (keeps track of the current position of monitored log files)
		sincedb_path => "c:/code/logstash-1.4.2/db"
		# For demo purposes we ingest the entire log file.  Typically you would leave the default ("end") to tail the log file
		start_position => "beginning"
	}
	file { 
		path => "c:/temp/jetty*.log" 
		type => "httpaccess"
		sincedb_path => "c:/code/logstash-1.4.2/db"
		start_position => "beginning"
	}
	file { 
		path => "c:/temp/aie-node*.log" 
		type => "aienode"
		sincedb_path => "c:/code/logstash-1.4.2/db"
		start_position => "beginning"
	}
}

filter {
	if [type] == "httpaccess" {
		grok { 
			# this is almost identical to the standard COMBINEDAPACHELOG - we just added some additional whitespace around ident & auth fields 
			match => [ "message", "%{IPORHOST:clientip} %{USER:ident}[ ]+%{USER:auth}[ ]+\[%{HTTPDATE:timestamp}\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent}" ]
		}
		# parse the agent string into something meaningfull
		useragent {
			source => "agent"
		}
	}
	if [type] == "aienode" {
		# every log entry starts with a timestamp: 2015-02-12 07:43:29,985 
		multiline {
		  pattern => "(^[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2},[0-9]{3})"
		  negate => true
		  what => "previous"
		}
		grok {
			# A custom regex/grok pattern for parsing AIE query logs
			#match => [ "message", "(?<datetime>[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2},[0-9]{3})\s+(?<loglevel_s>\S+)\s+(?<class_s>\S+)\s+\[(?<thread_s>\S+?)\]"  ]
			match => [ "message", "(?<datetime>[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2},[0-9]{3})[ ]+(?<loglevel_s>\S+)[ ]+(?<class_s>\S+)[ ]+\[(?<thread_s>\S+)\]"  ]
		}
		date {
			match => [ "datetime", "YYYY-MM-dd HH:mm:ss,SSS" ]
			timezone => "CET"
		}
		mutate {
			remove_field => ['datetime']
		}
	}	
	if [type] == "querylog" {
	  # Grok is a plugin for dismantling incoming unstructured events and turning them into structured data. 
		grok {
			# A custom regex/grok pattern for parsing AIE query logs
			match => [ "message", "[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2},[0-9]{3}\s+(?<loglevel_s>\S+)\s+(?<class_s>\S+)\s+\[(?<thread_s>\S+?)\] - node://(?<workflow_s>\S+) (?<client_id>\S+).*\[%{HTTPDATE:timestamp}\] %{QS:querystring_s} (?<errorcode_s>\S+) (?<rows_i>[0-9]+) (?<workflow_time_i>[0-9]+) (?<search_time_i>[0-9]+) (?<facet_time_i>[0-9]+) (?<result_time_i>[0-9]+) (?<highlight_time_i>[0-9]+)"  ]
		}

		# mutate is a plugin for manipulating fields/data in an event
		mutate {
			# the following is functionally equivalent to "querystring_s".replaceAll("\"", "")
			gsub => [
				"querystring_s", "\"", ""
			]
		}
		# kv is used to split a large string of data into field values pairs
		# e.g. query=test&language=simple -->  {"query": "test", "language": "simple"}
		  kv {
			#the input field for KV
			source => 'querystring_s'
			#the character/string to split on
			field_split => '&'
			# only interested in the q parameter
			include_keys => ['q']
		}

		urldecode {
			all_fields =>  true
		}
		
		mutate {
			# the 'q' parameter is the query 
			# set the query to the title
			rename => [ 'q', 'title' ]
			convert => [ "rows_i", "integer" ]
			convert => [ "workflow_time_i", "integer" ]
			convert => [ "search_time_i", "integer" ]
			convert => [ "facet_time_i", "integer" ]
			convert => [ "result_time_i", "integer" ]
			convert => [ "highlight_time_i", "integer" ]
		}
		if [title] =~ ".+" {
			# save the query under a facetable field as well
			mutate {
				replace => [ "query_s", "%{title}" ]
			}
		}
	}
	# following applied to all logs
	# apply the checksum plugin to generate an id
	checksum {
	}
	#assume timestamp follows standard http log format
	#this is applicable to http logs & aie query logs
	#12/Feb/2015:06:48:01 +0000
	date {
		match => [ "timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
	}
	# rename fields to match AIE conventions
	mutate {
		# the message is the full log entry - put it in the text field
		rename => [ 'message', 'text' ]
		# put timestamp in date field
		rename => [ '@timestamp', 'date' ]
		# type is the type of log - put in table
		rename => [ "type", "table" ]
		# use the checksum as the document id
		rename => [ 'logstash_checksum', '.id' ]
		# use dynamic field for host
		rename => [ "host", "host_s" ]
		rename => [ "path", "sourcepath" ]
		# remove unused fields
		remove_field => ['timestamp', '@version', 'minor', 'patch', 'os_name' ]

		#add dynamic field suffix to standard http access log fields
		rename => [ "clientip", "clientip_s" ]
		rename => [ "ident", "ident_s" ]
		rename => [ "auth", "auth_s" ]
		rename => [ "verb", "verb_s" ]
		rename => [ "request", "request_s" ]
		rename => [ "httpversion", "httpversion_s" ]
		rename => [ "rawrequest", "rawrequest_s" ]
		rename => [ "response", "response_s" ]
		rename => [ "bytes", "bytes_i" ]
		rename => [ "referrer", "referrer_s" ]
		rename => [ "agent", "agent_s" ]
		rename => [ "name", "browser_s"]
		rename => [ "major", "browser_version_s"]
		rename => [ "os", "os_s"]
		rename => [ "device", "device_s"]
	}
	if [sourcepath] =~ ".+" {
		grok {
			match => ["sourcepath","%{GREEDYDATA}/%{GREEDYDATA:filename}"]
		}
	}
}

output {
	# custom AIE output plugin
	aie {
		# The name of the node the AIE instance is running on
		node => "localhost"
		# The port the AIE instance is running on
		port => "17000"
		# The number of docs to buffer before POSTing to the index; defaults to 100
		# For ideal performance, set between 50 and 200.
		# for demo purposes, we set this to 1 so that we post log entries immediately
		buffer_size => 1
		# the session config - optional - if not set will use sensible defaults
		# for demo purposes, we set the documentBatchSize and commitInterval to 1 so we see results instantly
		session_config => {
			  sessionTimeout => -1
			  documentBatchSize => 1
			  commitInterval => 1
			  ingestWorkflowName => "ingest"
		}
	}
}
